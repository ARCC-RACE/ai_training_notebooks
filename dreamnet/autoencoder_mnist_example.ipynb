{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder MNIST\n",
    "\n",
    "In this code we are going to write a simple autoencoder to encode the mnist dataset into a latent space that can be accessed and used for storing the high level informaiton about the number. This will later be used to build an autoencoder to compress the state of the autonomous car on the track. TF 2.0 API documentation: https://www.tensorflow.org/versions/r2.0/api_docs/python/tf\n",
    "\n",
    "Also see: https://towardsdatascience.com/implementing-an-autoencoder-in-tensorflow-2-0-5e86126e9f7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# import the tensorflow keras dataset to use for the autoencoder. Normalize all the values to 0-1.0\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a class for the basic dense encoder and decoder\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, intermediate_dim, original_dim):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.hidden_layer = tf.keras.layers.Dense(\n",
    "      units=intermediate_dim,\n",
    "      activation=tf.nn.relu,\n",
    "      kernel_initializer='he_uniform'\n",
    "    )\n",
    "    self.output_layer = tf.keras.layers.Dense(\n",
    "      units=original_dim,\n",
    "      activation=tf.nn.sigmoid\n",
    "    )\n",
    "  \n",
    "  def call(self, code):\n",
    "    activation = self.hidden_layer(code)\n",
    "    return self.output_layer(activation)\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, intermediate_dim):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.hidden_layer = tf.keras.layers.Dense(\n",
    "      units=intermediate_dim,\n",
    "      activation=tf.nn.relu,\n",
    "      kernel_initializer='he_uniform'\n",
    "    )\n",
    "    self.output_layer = tf.keras.layers.Dense(\n",
    "      units=intermediate_dim,\n",
    "      activation=tf.nn.sigmoid\n",
    "    )\n",
    "    \n",
    "  def call(self, input_features):\n",
    "    activation = self.hidden_layer(input_features)\n",
    "    return self.output_layer(activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make an autoencoder from a decoder and encoder\n",
    "\n",
    "class Autoencoder(tf.keras.Model):\n",
    "  def __init__(self, intermediate_dim, original_dim):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.encoder = Encoder(intermediate_dim=intermediate_dim)\n",
    "    self.decoder = Decoder(intermediate_dim=intermediate_dim, original_dim=original_dim)\n",
    "  \n",
    "  def call(self, input_features):\n",
    "    code = self.encoder(input_features)\n",
    "    reconstructed = self.decoder(code)\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, original):\n",
    "  reconstruction_error = tf.reduce_mean(tf.square(tf.subtract(model(original), original)))\n",
    "  return reconstruction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loss, model, opt, original):\n",
    "  with tf.GradientTape() as tape:\n",
    "    gradients = tape.gradient(loss(model, original), model.trainable_variables)\n",
    "    gradient_variables = zip(gradients, model.trainable_variables)\n",
    "    opt.apply_gradients(gradient_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 10\n",
    "\n",
    "autoencoder = Autoencoder(intermediate_dim=64, original_dim=784)\n",
    "opt = tf.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "(training_features, _), (test_features, _) = tf.keras.datasets.mnist.load_data()\n",
    "training_features = training_features / np.max(training_features)\n",
    "training_features = training_features.reshape(training_features.shape[0],\n",
    "                                              training_features.shape[1] * training_features.shape[2])\n",
    "training_features = training_features.astype('float32')\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices(training_features)\n",
    "training_dataset = training_dataset.batch(batch_size)\n",
    "training_dataset = training_dataset.shuffle(training_features.shape[0])\n",
    "training_dataset = training_dataset.prefetch(batch_size * 4)\n",
    "\n",
    "writer = tf.summary.create_file_writer('tmp')\n",
    "\n",
    "with writer.as_default():\n",
    "  with tf.summary.record_if(True):\n",
    "    for epoch in range(epochs):\n",
    "      for step, batch_features in enumerate(training_dataset):\n",
    "        train(loss, autoencoder, opt, batch_features)\n",
    "        loss_values = loss(autoencoder, batch_features)\n",
    "        original = tf.reshape(batch_features, (batch_features.shape[0], 28, 28, 1))\n",
    "        reconstructed = tf.reshape(autoencoder(tf.constant(batch_features)), (batch_features.shape[0], 28, 28, 1))\n",
    "        tf.summary.scalar('loss', loss_values, step=step)\n",
    "        tf.summary.image('original', original, max_outputs=10, step=step)\n",
    "        tf.summary.image('reconstructed', reconstructed, max_outputs=10, step=step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
